% !Rnw weave = knitr

\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}


\usepackage[utf8]{inputenc}
\usepackage[cyr]{aeguill}
\usepackage[francais]{babel}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section*{Exercice 1}







On centre tout d'abord la matrice X à l'aide de la matrice de centrage Q8, cela nous servira plus tard :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{X} \hlkwb{=} \hlstd{Q8} \hlopt{%*%} \hlstd{X}
\end{alltt}
\end{kframe}
\end{knitrout}
On calcule la matrice des distances euclidiennes D2 :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{D2} \hlkwb{=} \hlkwd{as.matrix}\hlstd{(}\hlkwd{dist}\hlstd{(X))}
\hlstd{D2} \hlkwb{=} \hlstd{D2}\hlopt{^}\hlnum{2}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##       1     2     3     4     5     6     7     8
## 1  0.00 37.25 67.25  1.00  1.00 55.25  1.25 58.25
## 2 37.25  0.00  4.50 48.25 31.25  2.50 36.50  2.50
## 3 67.25  4.50  0.00 81.25 58.25  1.00 65.00  1.00
## 4  1.00 48.25 81.25  0.00  2.00 67.25  1.25 72.25
## 5  1.00 31.25 58.25  2.00  0.00 46.25  0.25 51.25
## 6 55.25  2.50  1.00 67.25 46.25  0.00 52.00  2.00
## 7  1.25 36.50 65.00  1.25  0.25 52.00  0.00 58.00
## 8 58.25  2.50  1.00 72.25 51.25  2.00 58.00  0.00
\end{verbatim}
\end{kframe}
\end{knitrout}
Après avoir obtenu cette matrice, on peut calculer la matrice des produits scalaires de 2 façons :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{W} \hlkwb{=} \hlstd{X} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}
\end{alltt}
\end{kframe}
\end{knitrout}

Mais normalement, on n'aurait pas accès à X directement, il faut la déduire de la matrice des distances euclidiennes :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{W} \hlkwb{=} \hlstd{(}\hlopt{-}\hlnum{1}\hlopt{/}\hlnum{2}\hlstd{)}\hlopt{*}\hlstd{Q8} \hlopt{%*%} \hlstd{D2} \hlopt{%*%} \hlstd{Q8}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]
## [1,]  13.50  -8.78 -16.56  16.25  11.07 -13.81  12.44 -14.12
## [2,]  -8.78   6.19  11.16 -11.03  -7.71   8.91  -8.84  10.10
## [3,] -16.56  11.16  20.63 -20.31 -14.00  16.88 -15.87  18.07
## [4,]  16.25 -11.03 -20.31  20.00  13.82 -16.56  15.69 -17.87
## [5,]  11.07  -7.71 -14.00  13.82   9.63 -11.25  11.00 -12.56
## [6,] -13.81   8.91  16.88 -16.56 -11.25  14.13 -12.62  14.32
## [7,]  12.44  -8.84 -15.87  15.69  11.00 -12.62  12.63 -14.43
## [8,] -14.12  10.10  18.07 -17.87 -12.56  14.32 -14.43  16.50
\end{verbatim}
\end{kframe}
\end{knitrout}


Il reste à vérifier que W soit semi définie-positive, pour savoir si la matrice des distances était bien euclidienne :

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{eigen}\hlstd{(W}\hlopt{/}\hlnum{8}\hlstd{)}\hlopt{$}\hlstd{values}
\end{alltt}
\begin{verbatim}
## [1]  1.393257e+01  2.197736e-01  1.376168e-15 -4.183151e-17 -1.187592e-16
## [6] -1.337349e-16 -2.307438e-16 -7.506684e-16
\end{verbatim}
\end{kframe}
\end{knitrout}


W est bien semi définie-positive, car ses valeurs propres sont positives ou nulles. Nous avons mis à zéro les valeurs négatives (erreurs d'arrondis de calcul du processeur), et diagonalisé les valeurs propres.



Matrice L des valeurs propres :

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##          [,1]      [,2]         [,3] [,4] [,5] [,6] [,7] [,8]
## [1,] 13.93257 0.0000000 0.000000e+00    0    0    0    0    0
## [2,]  0.00000 0.2197736 0.000000e+00    0    0    0    0    0
## [3,]  0.00000 0.0000000 1.376168e-15    0    0    0    0    0
## [4,]  0.00000 0.0000000 0.000000e+00    0    0    0    0    0
## [5,]  0.00000 0.0000000 0.000000e+00    0    0    0    0    0
## [6,]  0.00000 0.0000000 0.000000e+00    0    0    0    0    0
## [7,]  0.00000 0.0000000 0.000000e+00    0    0    0    0    0
## [8,]  0.00000 0.0000000 0.000000e+00    0    0    0    0    0
\end{verbatim}
\end{kframe}
\end{knitrout}


Ensuite nous avons calculé la matrice des vecteurs propres, et l'avons normée correctement :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{V} \hlkwb{=} \hlkwd{eigen}\hlstd{(W)}\hlopt{$}\hlstd{vectors}
\hlstd{V} \hlkwb{=} \hlstd{V}\hlopt{*}\hlkwd{sqrt}\hlstd{(}\hlnum{8}\hlstd{)}  \hlcom{# on norme les vectors propres au sens de (1/n)*I}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{round}\hlstd{(V,}\hlnum{2}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
## [1,]  0.97  1.29  0.00  0.00  0.00  0.00  0.00  2.32
## [2,] -0.66  0.63  1.57 -1.70 -0.47 -1.08 -0.65 -0.07
## [3,] -1.22 -0.38 -0.92  0.83  0.02 -1.15 -1.73  0.72
## [4,]  1.20  0.16 -0.93 -1.25  0.85  0.62 -1.63 -0.59
## [5,]  0.83 -0.52 -0.67 -0.26 -2.54 -0.13 -0.21 -0.06
## [6,] -0.99 -1.51 -0.23 -1.30  0.07  1.15  0.29  1.25
## [7,]  0.94 -1.09 -0.54 -0.62  0.75 -1.94  0.95  0.21
## [8,] -1.07  1.43 -1.74 -0.83 -0.16 -0.20  0.95 -0.34
\end{verbatim}
\end{kframe}
\end{knitrout}


Cela nous permet d'en déduire une version approximée par AFTD de X :
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Xaftd} \hlkwb{=} \hlstd{V} \hlopt{%*%} \hlkwd{sqrt}\hlstd{(L)}
\end{alltt}
\end{kframe}
\end{knitrout}

Voici une comparaison graphique de X et Xaftd :


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-14-1} 

\end{knitrout}


On constate que les résultats sont très proches (aux dissimilarités isométriques près).

Voici une fonction qui fait l'AFTD d'un tableau de distance, et fait un plot du résultat :

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{AFTD} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{D}\hlstd{)\{}
    \hlstd{D} \hlkwb{=} \hlkwd{as.matrix}\hlstd{(D)}
    \hlstd{D} \hlkwb{=} \hlstd{D}\hlopt{^}\hlnum{2}
    \hlstd{Q} \hlkwb{=} \hlkwd{diag}\hlstd{(}\hlkwd{nrow}\hlstd{(D))}\hlopt{-}\hlkwd{matrix}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{nrow}\hlstd{(D),} \hlkwd{nrow}\hlstd{(D))}\hlopt{/}\hlkwd{nrow}\hlstd{(D)}
    \hlstd{W} \hlkwb{=} \hlstd{(}\hlopt{-}\hlnum{1}\hlopt{/}\hlnum{2}\hlstd{)}\hlopt{*}\hlstd{Q}\hlopt{%*%}\hlstd{D}\hlopt{%*%}\hlstd{Q}
    \hlstd{V} \hlkwb{=} \hlkwd{eigen}\hlstd{(W}\hlopt{/}\hlkwd{nrow}\hlstd{(W))}\hlopt{$}\hlstd{vectors}
    \hlstd{V} \hlkwb{=} \hlstd{V}\hlopt{*}\hlkwd{sqrt}\hlstd{(}\hlkwd{nrow}\hlstd{(D))}
    \hlstd{L} \hlkwb{=} \hlkwd{eigen}\hlstd{(W}\hlopt{/}\hlkwd{nrow}\hlstd{(W))}\hlopt{$}\hlstd{values}
    \hlstd{L[L}\hlopt{<}\hlnum{0}\hlstd{]} \hlkwb{=} \hlnum{0}
    \hlstd{L} \hlkwb{=} \hlkwd{diag}\hlstd{(L)}

    \hlstd{C} \hlkwb{=} \hlstd{V}\hlopt{%*%}\hlkwd{sqrt}\hlstd{(L)}
    \hlkwd{plot}\hlstd{(C)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{verbatim}


1. distX = as.matrix(X)
    distX = distX ^2
  
2. Méthode 1 
  XC = scale(X, scale=T)
  W = XC\%*\%t(XC)
  
  Méthode 2 
  QN = diag(nrow(X)) - matrix(1, nrow(X), nrow(X))/nrow(X)
  W = -1/2*QN\%*\%distX\%*\%QN
  
3. Pour vérifier si elle est définie semi-positive, il suffit de vérifier que les valeurs propres soient positives ce qui est le cas car on peut considérer que -1.37*10^-17 et -2.45*10^-16 sont des valeurs nulles. 
eigen(W)

4. L = eigen(W)$values
  L = diag(nrow(X))*L
  
  V = eigen(W)$vectors
  
5. C = V\%*\%sqrt(L)
  pas oublier de retirer les NaN
  
  plot(C)
  idem à biplot(princomp(X))
  
\end{verbatim}
  
\section*{Exercice 2}

\begin{verbatim}
m = as.vector(mutation)
b = cmdscale(mutation, 2, T)
c = as.vector(dist(b$points))
plot(b,c) problème mais on est pas loin 
qualité à calculer avec les valeurs propres b[,1]$eigen etc... / sum 

on refait de même avec cmdscale(mutation, 3, T) jusqu'à 5 

\end{verbatim}

\section*{Exercice 3} 



library(cluster)
clusplot 
\subsection*{Iris}

\begin{verbatim}
iris = iris[ ,1:4]
res2 = kmeans(iris, 2)
plot(iris, col= res2$cluster)
clusplot(iris, res2$cluster)i
res2 = kmeans(iris, 3)
> plot(iris, col= res2$cluster)

2 types différents : 143 ou 78.9 pour l'inertie des classes (tot.withinss)

$for(j in 2:10){
  for(i in 1:100){
    test[j, i] = kmeans(iris, j)$tot.withinss
  }
}$
apply(test, 2, min)
La solution qui apparait en 3 classes n'est pas flagrante avec le tableau des minimums des inerties. La méthode du coude ne fonctionne pas très bien, elle ne fait pas apparaitre de coude. Le minimum d'inertie de  fait que diminuer en fonction du nombre de classes. 
Une solution serait de pénaliser un grand nombre de classes par le nombre d'individus présents dans la classe.
\end{verbatim}

\subsection*{Crabs}

library(MASS)


\subsection*{Mutations}
\begin{verbatim}
res = kmeans(mutations2, 2)
plot(cmdscale(mutations), col=res$cluster)

Avec 3 vert au milieu des noirs 

4 cluster seulement un point dans le dernier

tableau de contingence pour comparer les partitions table(res$cluster, res2$cluster)
\end{verbatim}



\end{document}
