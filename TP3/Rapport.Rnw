\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[cyr]{aeguill}
\usepackage[francais]{babel}
\usepackage{amsmath}
%\usepackage{bbold}

\begin{document}

\title{TP3: Discrimination, théorie bayésienne de la décision}
\author{Jean Baptiste Vivier - Marion Depuydt}
\maketitle
\section*{Classifieur euclidien, K plus proches voisins}

\subsection*{Introduction}

Dans ce TP, nous allons étudier des méthodes de classification automatique telles que le classifieur euclidien et les K plus proches voisins. Nous allons aussi dans un deuxième temps étudier la théorie bayésienne afin de trouver la règle de décision optimale.

\subsection*{Programmation}

Voir code en annexe


\subsection*{Evaluation des performances}

\subsubsection*{Centre de gravité $ \mu_k $ et matrice de covariance $ \sum_k $ }

\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Synth1} & $\mu_k$ & $\sum_k$ & $\pi_k$  \\
\hline
40 & $k=1$ & (-1.904681,0.698841) & $\begin{pmatrix}
   0.7701334  & 0.2566214 \\
   0.2566214 & 1.0798685 
\end{pmatrix}$
  & 0.55 \\
\cline{2-3}
 & $k=2$  & (0.8808594, 0.8587835) & $\begin{pmatrix}
  1.41546550  & -0.01011499 \\
  -0.01011499 & 1.22191696 
\end{pmatrix}$ & 0.45 \\
\hline
100 & $k=1$ & (-1.808233,1.057774) & $\begin{pmatrix}
   1.50880197  & 0.07097749 \\
   0.07097749 & 0.95833356 
\end{pmatrix}$
  & 0.53 \\
\cline{2-3}
 & $k=2$  & (0.9831412 1.1610775) & $\begin{pmatrix}
  0.84677155  & 0.04545945 \\
  0.04545945 & 0.71631862 
\end{pmatrix}$ & 0.48 \\
\hline
500 & $k=1$ & (-1.9101695, 0.9359349) & $\begin{pmatrix}
   1.16050554  & 0.03149239 \\
   0.03149239 & 0.87727376 
\end{pmatrix}$
  & 0.48 \\
\cline{2-3}
 & $k=2$  & (0.9979001, 0.9843533) & $\begin{pmatrix}
  0.91978725  & -0.03484383 \\
  -0.03484383 & 0.95416768 
\end{pmatrix}$ & 0.52 \\
\hline
1000 & $k=1$ & (-1.998992, 1.008186) & $\begin{pmatrix}
   1.03969441  & -0.07115634 \\
   -0.07115634 & 0.97107695 
\end{pmatrix}$
  & 0.488 \\
\cline{2-3}
 & $k=2$  & (1.0908900 0.9837324) & $\begin{pmatrix}
  1.00246869  & -0.02488749 \\
  -0.02488749 & 1.02228782 
\end{pmatrix}$ & 0.512 \\
\hline
\end{tabular}\\ \\

Nous remarquons que plus l'échantillon est grand plus $\mu_1 \to \begin{pmatrix} -2 \\ 1 \end{pmatrix}$ et $\mu_2 \to \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
De même, $\sum_1 = \sum_2 \to \begin{pmatrix}
  1  & 0 \\
  0 & 1  
\end{pmatrix}$ et $\pi_1 = \pi_2 \to$ 0.5.

\subsection*{Intervalle de confiance}


Soit $E=\frac{1}{m}\sum 1_{z_i\ne z_i}$, $T_i=1_{z_i\ne z_i}$ suit une loi de Bernouilli de paramètre $\varepsilon$ car $P(T_i=1)=E(E)=E(\frac{1}{m}\overset{m}{\underset{i}{\sum}}T_i)=\frac{1}{m}E(\overset{m}{\underset{i}{\sum}}{T_i})=m\overset{m}{\underset{i}{\sum}} E(T_1)=\frac{m \varepsilon}{m}=\varepsilon$.



$\bar{E}$ suit une loi binomiale puisqu'il s'agit de la répétition de E N fois. $\bar{E} \sim \mathcal{B}(N, \varepsilon)$. De plus, $E(\bar{E})=N\varepsilon$ et $Var(\bar{E}=N\varepsilon(1-\varepsilon))$. Elle peut être approchée par une loi normale car on suppose que N est grand.
$\bar{E} \sim \mathcal{N}(E(\bar{E}),Var(\bar{E}))$
$\bar{E} \sim \mathcal{N}(N\varepsilon,N\varepsilon(1-\varepsilon))$ et $E\sim \mathcal{N}(\varepsilon,\varepsilon(1-\varepsilon))$.

On ne connait pas la variance. On sait que l'on a pour une loi gaussienne E : $\frac{\bar{E}-\mu}{S^*/\sqrt{N}} \sim \tau_{N-1}$ On peut en déduire un intervalle de confiance pour l'espérance de E, comme les taux d'erreur $E_j$ sont indépendants. 

On va chercher $\mu$ tel que : $P(\frac{\bar{E}-\mu}{S^*/\sqrt{N}}) < 1-\alpha$

On obtient après calculs :
$IC = [\bar{E} - t_{N-1;1-\alpha/2}\frac{S^*}{\sqrt{N}}, \bar{E} + t_{N-1;1-\alpha/2}\frac{S^*}{\sqrt{N}}]$


\subsection*{Sript}

Vous trouverez le script en annexes (Calcul d'erreurs).

\begin{tabular}{|c|c|c|c|c|}
\hline
Synth1 & $\varepsilon apprentissage$ & $\varepsilon test$ & IC apprentissage & IC test  \\
\hline
40 & 0.1000000 & 0.1230769 & [0.03,0.18] & [0, 0.23] \\
\hline
100 & 0.09242424 & 0.12500000 & [0.04545455 , 0.1212121] & [0.08823529, 0.2352941] \\
\hline
500 &  0.07717718 & 0.06676647& [0.06606607, 0.09009009] & [0.04191617, 0.08982036]\\
\hline
1000 & 0.06809309  & 0.06407186 & [0.05555556, 0.07657658] & [0.05089820, 0.08982036] \\
\hline
\end{tabular}\\ \\

Nous remarquons que plus le nombre de l'échantillon est élévé plus le classifieur est fiable. En effet, le taux d'erreur diminue ainsi que l'intervalle de confiance. 

\subsection*{Nombre optimal de voisins}
 Si nous utilisons le même set d'apprentissage et de validation nous obtenons, toujours K égal au plus grand nombre possible car plus K est grand plus la règle de décision est précise et donc optimisé. 

\subsection*{Script pour KPPV}

Vous trouverez le script en annexes (Calcul d'erreurs de KPPV).

\begin{tabular}{|c|c|c|c|c|}
\hline
Synth1 & $\varepsilon apprentissage$ & $\varepsilon test$ & IC apprentissage & IC test  \\
\hline
40 & 0.1125  & 0.1650 & [0, 0.35] & [0, 0.40] \\
\hline
100 & 0.090 & 0.108 & [0.04, 0.18] & [0, 0.20] \\
\hline
500 &  0.070 & 0.076 & [0.052, 0.092] & [0.040, 0.104]\\
\hline
1000 & 0.0637 & 0.0680 & [0.054, 0.08] & [0.040, 0.10] \\
\hline
\end{tabular}\\ \\

Nous remarquons que cette fois aussi, plus l'échantillon est grand plus le taux d'erreur diminue et l'intervalle de confiance de restreint. Nous notons aussi que ces valeurs sont quasiment identiques à celle de classifieur euclidien dans notre cas, le classifieur euclidien décompose aussi bien le problème en moins de temps.

\subsection*{Paramètres $\mu_k$ et $\sum_k$ de Synth2}

\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Synth2} & $\mu_k$ & $\sum_k$ & $\pi_k$  \\
\hline
1000 & $k=1$ & (-4.055942,1.011496) & $\begin{pmatrix}
   1.01455973  & 0.01845083 \\
   0.01845083 & 0.93975424 
\end{pmatrix}$
  & 0.488 \\
\cline{2-3}
 & $k=2$  & (4.028957, 1.067821) & $\begin{pmatrix}
  4.9533975  & 0.1079094 \\
  0.1079094 & 5.0221245 
\end{pmatrix}$ & 0.512 \\
\hline
\end{tabular}

\subsection*{Estimations pontuelles et IC de Synth2}

\begin{tabular}{|c|c|c|c|c|}
\hline
Synth2 & $\varepsilon apprentissage$ & $\varepsilon test$ & IC apprentissage & IC test  \\
\hline
1000 & 0.02214715 & 0.02470060 & [0.01801802, 0.02852853] & [0.01197605, 0.03293413] \\
\hline
\end{tabular}

Nous remarquons que pour un échantillon de même taille, les taux d'erreurs et les intervaux de confiances sont plus faibles pour Synth2. Nous remarquons qu'une des classes de Synth2 a une dispersion beaucoup plus faibles que les classes de Synth1. Cela permet au classifieur euclidien d'être plus efficace. 

<<echo=FALSE>>=
load("data.RData")
@

<<echo=F>>=
par(mfrow=c(1,2))
plot(Synth2[,1] ~ Synth2[,2], col=Synth2[,3], xlab="x",ylab="y")
title("Synth2 1000")
plot(synth1000[,1] ~ synth1000[,2], col=synth1000[,3], xlab="x",ylab="y")
title("Synth1 1000")
@

\section*{Règle de Bayes}

\subsection*{Distributions marginales}
X\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(\mu_1, \sum_1)$ et X\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(\mu_2, \sum_2)$
ce qui donne respectivement \newline
$f_1(x)= \frac{1}{2\pi}e^{-1/2*((x_1+2)^2+(x_2-1)^2)}$ et
$f_2(x)= \frac{1}{2\pi}e^{-1/2*((x_1-1)^2+(x_2-1)^2)}$ \newline
$\sum_1$ et $\sum_2$ sont diagonales ce qui implique que $X_1$, ..., $X_k$ sont indépendantes Nous pouvons par conséquent écrire que $f_1=f_{11}*f_{12}$ et $f_2=f_{21}*f_{22}$. \newline 
De plus, nous savons que les $f_{11}$ et $f_{12}$ sont des lois normales car ce sont des sous vecteurs d'un vecteur gaussien. \newline
Nous pouvons donc poser
$X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(a_1, b_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(a_2, b_2)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(c_1, d_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(c_2, d_2)$. \newline
En résolvant le système nous obtenons: $X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(-2, 1)$ , $X^1$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$ ainsi que $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(1, 1)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$

Pour le jeu de données Synth2, nous obtenons $f_1(x)= \frac{1}{2\pi}e^{-1/2*((x_1+4)^2+(x_2-1)^2)}$
et $f_2(x)= \frac{1}{10\pi}e^{-1/10*((x_1-4)^2+(x_2-1)^2)}$
De même tous sous vecteurs d'un vecteur gaussien suit une loi normale et sont indépendants car les matrices $\sum_1$ et $\sum_2$ sont diagonales. 
En procédant de la même manière, nous obtenons $X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(-4, 1)$ , $X^1$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$ ainsi que $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(4, 5)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 5)$

\subsection*{Courbes d'iso-densité}

L'équation de la courbe d'iso-densité de la classe 1 est $(x - \mu_1)^t\sum_1^{-1}(x-\mu_1)=c$ où c est une constante. Après avoir effectué les calculs nous obtenons, $(x_1-2)^2+(x_2-1)^2=c_1$ or l'équation d'un cercle de centre (a,b) et de rayon s'écrit $(x_1-a)^2+(x_2-b)^2=r^2$. 
Nous en déduisons donc que la courbe d'iso-densité de la classe 1 est un cercle de centre (-2,1) et de rayon $\sqrt{c_1}$.

Nous procédons de même pour déterminer que la courbe d'iso-densité de la classe 2 est un cercle de centre (1, 1) et de rayon $\sqrt{c_2}$.Nous remarquons que le centre des cercles correspond à $\mu$. 

Nous pouvons avoir une idée plus précise de la valeur $c_1$ en résolvant l'équation $f_1(x)=K_1$ où $K_1$ est une constante. Nous trouvons que $c_1=-2ln(2\pi K_1)$ et de la même manière $c_2=-2ln(2\pi K_2)$

Nous procédons de même pour le set données Synth2, nous obtenons ainsi que la courbe d'iso densité de la classe 1 est un cercle de centre (-4,1) et de rayon $\sqrt{-2ln(2\pi K_3)}$ et que celle de la classe est aussi un cercle de centre (4, 1) et de rayon $\sqrt{-10ln(10\pi K_4)}$.

\subsection*{Règle de Bayes}
La règle de Bayes s'écrit de la manière suivante:
$\delta^{*}(x) =
\begin{cases} 
  a_{1} & si \frac{f_{1}(x)}{f_{2}(x)} > \frac{\pi_{2}}{\pi{1}} \\
  a_{2} & sinon.
\end{cases}$\\ \\

Après simplification nous obtenons, $\delta^{*}(x) = \begin{cases} 
  a_{1} & si  \> x_1 < \frac{ln(\pi_1)-ln(\pi_2)}{3} - \frac{1}{2} \\
  a_{2} & sinon.
\end{cases}$\\ \\

Or d'après l'énonce $\pi_1 = 0.5$ donc $\pi_2= 0.5$ aussi pour que la somme des proportions des classes soient égales à 1 ce qui simplifie l'équation:\\ 
$\delta^{*}(x) = \begin{cases} 
  a_{1} & si  \> x_1 < - \frac{1}{2} \\
  a_{2} & sinon.
\end{cases}$\\ \\

Pour Synth2, après symplification nous obtenons: \\ 
$\delta^{*}(x) = \begin{cases} 
  a_{1} & si  \> (x_1+4)^2+(x_2-1)^2 < -\frac{5}{4}ln(\frac{\pi_2}{5\pi_1}) \\
  a_{2} & sinon.
\end{cases}$\\ \\

\subsection*{Frontières de décision}
Vous trouverez ci dessous les 4 graphes dans le plan avec la frontière de décision trouvée à la question précédente.
<<echo=FALSE>>=
load("data.RData")
@

<<echo=F>>=
par(mfrow=c(2,2))
plot(synth1[,1] ~ synth1[,2], col=synth1[,3], xlab="X1",ylab="X2")
abline(a = -1/2, b=0, col = "blue")
title("Synth1 40")
plot(synth100[,1] ~ synth100[,2], col=synth100[,3], xlab="X1",ylab="X2")
abline(a = -1/2, b=0, col = "blue")
title("Synth1 100")
plot(synth500[,1] ~ synth500[,2], col=synth500[,3], xlab="X1",ylab="X2")
abline(a = -1/2, b=0, col = "blue")
title("Synth1 500")
plot(synth1000[,1] ~ synth1000[,2], col=synth1000[,3], xlab="X1",ylab="X2")
abline(a = -1/2, b=0, col = "blue")
title("Synth1 1000")
@
\subsection*{Erreur de Bayes}

$\alpha$ = $\mathcal{P}$($\delta^{*}$(\textit{x}) = $a_{2}\mid\omega_{1}$) et
$\beta$ = $\mathcal{P}$($\delta^{*}$(\textit{x}) = $a_{1}\mid\omega_{2}$). Pour les calculer, vous trouverez les deux fonctions que nous avons utilisés en annexes (Estimateur $\alpha$ et $\beta$). \\

\begin{tabular}{|c|c|c|}
\hline
Synth1 & $\alpha$ & $\beta$ \\
\hline
40 & 0.09090909 & 0.09090909 \\
\hline
100 & 0.1320755 & 0.05660377 \\
\hline
500 & 0.08333333 & 0.07083333 \\
\hline
1000 & 0.07786885 & 0.05737705 \\
\hline
\end{tabular}\\ \\

<<>>=
mu <- ceuc.app(Xapp, zapp)
par(mfrow=c(1,2))
myfront.ceuc(mu, X, z, ens)
myfront.kppv(Xapp, zapp, 5, X, z, ens)
@

Nous voyons que la frontière est pratiquement la même d'après le classifieur euclidien à $x=-\frac{1}{2}$ par contre le classifieur KPPV qui permet de claissfier des problèmes non linéaires trace un chemin dont la moyenne semble aussi correspondre à $-\frac{1}{2}$.

\section*{Conclusion}
Nous avons implémenter deux méthodes de classification supervisée. Le classifieur euclidien qui fonctionne mieux quand la dispersion des classes est faible et le classifieur des K plus proches voisins qui demandent plus de calculs et de bien choisir K.
Puis nous avons étudié la règle de Bayes en se basant sur des paramètres connus qui permet de contrôler non pas le nombre d'erreur mais les risques associés à chaque type d'erreur afin de minimiser celui que l'on souhaite. 

\section*{Annexes}

\subsection*{Classifieur euclidien: fonction d'apprentissage}
<<eval=FALSE, fig.height=1>>=
ceuc.app <- function(Xapp, zapp){
  apply(Xapp, 2, by, zapp, mean)
}
@
\subsection*{Classifieur euclidien: fonction d'évaluation}
<<eval=FALSE>>=
ceuc.val <- function(mu, Xtst){
  sol <- matrix(nrow = nrow(Xtst))
  dist <- matrix(ncol = nrow(mu), nrow = nrow(Xtst))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(mu)){
      dist[i,j] <- sqrt(sum((Xtst[i,]-t(mu[j,]))^2))
    }
  }
  # choisir la colonne de la classe
  min <- apply(dist, 1, min)
  ntst <- vector('numeric', nrow(dist))
  for(i in 1:nrow(dist)){
    for(j in 1:ncol(dist)){
      if(min[i]==dist[i,j]){
        ntst[i] = j
      }
    }
  }
  ntst
}
@
\subsection*{KPPV: fonctions d'apprentissage du nombre optimal de voisins K}
<<eval=FALSE>>=
kppv.app <- function(Xapp, zapp, Xval, zval, nppv)
{
  for(i in nppv){
    min <- i
    erreur_opt <- 1
    tmp <- kppv.val(Xapp, zapp, i, Xval)
    erreur <- sum((tmp == zval)==TRUE)/length(zval)
    if(erreur_opt > erreur){
      erreur_opt <- erreur
      min <- i
    }
  }
  min
}
@
\subsection*{KPPV: fonctions d'évaluation suivant un K donné}
<<eval=FALSE>>=
kppv.val <- function(Xapp, zapp, K, Xtst)
{
  dist <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(Xapp)){
      dist[j,i] <- sqrt(sum((Xtst[i,]-t(Xapp[j,]))^2))
    }
  }
  tmp <- apply(dist, 2, order)
  tmp2 <- tmp
  for(i in 1:ncol(tmp)){
    tmp2[,i] <- zapp[tmp[,i]]
  }
  tmp2 <- tmp2[1:K,]
  round(apply(tmp2, 2, mean))
}
@
\subsection*{Estimateur $\alpha$ et $\beta$}
<<eval=FALSE>>=
estimateurAlpha <- function(data){
  sum(apply(data, 1,
  function(row) {
    if(row[3] == 1 && row[1] > -0.5) 
      { return(1) } 
    return(0) }
  )) / sum(data[,3] == 1)
}

estimateurBeta <- function(data){
  sum(apply(data, 1,
            function(row) {
              if(row[3] == 2 && row[1] < -0.5) 
              { return(1) } 
              return(0) }
  )) / sum(data[,3] == 1)
}
@
\subsection*{Calcul d'erreurs}
<<eval=FALSE>>=
kppv.val <- function(Xapp, zapp, K, Xtst)
{
  dist <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(Xapp)){
      dist[j,i] <- sqrt(sum((Xtst[i,]-t(Xapp[j,]))^2))
    }
  }
  tmp <- apply(dist, 2, order)
  tmp2 <- tmp
  for(i in 1:ncol(tmp)){
    tmp2[,i] <- zapp[tmp[,i]]
  }
  tmp2 <- tmp2[1:K,]
  round(apply(tmp2, 2, mean))
}
@
\subsection*{Estimateur $\alpha$ et $\beta$}
<<eval=FALSE>>=
calculErreurs <- function(X, z){
  res <- matrix(nrow=20, ncol=2)
  for(i in 1:20){    
    donn.sep <- separ1(X, z)
    mu <- ceuc.app(donn.sep$Xapp, donn.sep$zapp)
    # la première colonne le taux d'erreur sur l'apprentissage 
    # et la deuxième colonne sur le test
    res[i,1] <-tauxErreur(ceuc.val(mu,donn.sep$Xapp),donn.sep$zapp)
    res[i,2] <-tauxErreur(ceuc.val(mu,donn.sep$Xtst),donn.sep$ztst)
  }
  ret <- NULL
  ret$taux <- res
  ret$estPonc <- estimationPonctuelle(ret$taux)
  ret$IC <- intervalleDeConfiance(ret$taux)  
  ret
}

# Permet d'obtenir la moyenne d'erreur pour chaque taux
estimationPonctuelle <- function(err){
  res <- apply(err, 2, mean)
  res
}

# Permet d'obtenir une estimation de l'IC pour chaque taux
intervalleDeConfiance <- function(err){
  res <- matrix(nrow=2, ncol=2)
  res[,1] <- apply(err, 2, min)
  res[,2] <- apply(err, 2, max)
  res
}

# Calcul le taux d'éléments qui ne sont pas dans la bonne classe
tauxErreur <- function(eti1, eti2){
  e = 0.0
  for(i in 1:length(eti1)){
    if(eti1[i] != eti2[i]){
      e = e + 1
    }
  }
  e / length(eti1)
}
@
\subsection*{Calcul d'erreurs de KPPV}
<<eval=FALSE>>=
Kppv.erreur <- function(X, z){ 
  for(i in 1:20){
    donn <- separ2(X, z)
    opt <- kppv.app(donn$Xapp, donn$zapp, donn$Xval, donn$zval, 2:10)
    res[i,1] <- tauxErreur(kppv.val(as.matrix(donn$Xapp), 
                donn$zapp, opt, as.matrix(donn$Xapp)), donn$zapp)
    res[i,2] <- tauxErreur(kppv.val(as.matrix(donn$Xapp), 
                donn$zapp, opt, as.matrix(donn$Xtst)), donn$ztst)
  }
  ret <- NULL
  ret$taux <- res
  ret$estPonc <- estimationPonctuelle(ret$taux)
  ret$IC <- intervalleDeConfiance(ret$taux)  
  ret
}
@
\end{document}