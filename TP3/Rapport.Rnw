\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[cyr]{aeguill}
\usepackage[francais]{babel}
\usepackage{amsmath}

\begin{document}

\title{TP3: Discrimination, théorie bayésienne de la décision}
\author{Jean Baptiste Vivier - Marion Depuydt}
\maketitle
\section*{Classifieur euclidien, K plus proches voisins}



\subsection*{Programmation}

Voir code en annexe


\subsection*{Evaluation des performances}

\subsubsection*{Centre de gravité $ \mu_k $ et matrice de covariance $ \sum_k $ }
\section*{Règle de Bayes}

\subsection*{Distributions marginales}
X\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(\mu_1, \sum_1)$ et X\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(\mu_2, \sum_2)$
ce qui donne respectivement \newline
$f_1(x)= \frac{1}{(2\pi)^{p/2}}e^{-1/2*((x+2)^2+(y-1)^2)}$ et
$f_2(x)= \frac{1}{(2\pi)^{p/2}}e^{-1/2*((x-1)^2+(y-1)^2)}$ \newline
$\sum_1$ et $\sum_2$ sont diagonales ce qui implique que $X_1$, ..., $X_k$ sont indépendantes Nous pouvons par conséquent écrire que $f_1=f_{11}*f_{12}$ et $f_2=f_{21}*f_{22}$. \newline 
De plus, nous savons que les $f_{11}$ et $f_{12}$ sont des lois normales car ce sont des sous vecteurs d'un vecteur gaussien. \newline
Nous pouvons donc poser
$X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(a_1, b_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(a_2, b_2)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(c_1, d_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(c_2, d_2)$. \newline
En résolvant le système nous obtenons: $X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(-2, 1)$ , $X^1$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$ ainsi que $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(1, 1)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$

\subsection*{Courbes d'iso-densité}

L'équation de la courbe d'iso-densité de la classe 1 est $(x - \mu_1)^t\sum_1^{-1}(x-\mu_1)=c$ où c est une constante. Après avoir effectué les calculs nous obtenons, $(x-2)^2+(y-1)^2=c$ or l'équation d'un cercle de centre (a,b) et de rayon s'écrit $(x-a)^2+(y-b)^2=r^2$. 
Nous en déduisons donc que la courbe d'iso-densité de la classe 1 est un cercle de centre (-2,1) et de rayon $\sqrt{c}$.

Nous procédons de même pour déterminer que la courbe d'iso-densité de la classe 2 est un cercle de centre (1, 1) et de rayon $\sqrt{c}$.Nous remarquons que le centre des cercles correspond à \mu. 

\begin{verbatim}



\end{verbatim}





\section*{Annexe}

\subsection*{Classifieur euclidien: fonction d'apprentissage}
<<eval=FALSE>>=
ceuc.app <- function(Xapp, zapp){
  apply(Xapp, 2, by, zapp, mean)
}
@
\subsection*{Classifieur euclidien: fonction d'évaluation}
<<eval=FALSE>>=
ceuc.val <- function(mu, Xtst){
  sol <- matrix(nrow = nrow(Xtst))
  dist <- matrix(ncol = nrow(mu), nrow = nrow(Xtst))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(mu)){
      dist[i,j] <- sqrt(sum((Xtst[i,]-t(mu[j,]))^2))
    }
  }
  # choisir la colonne de la classe
  min <- apply(dist, 1, min)
  ntst <- vector('numeric', nrow(dist))
  for(i in 1:nrow(dist)){
    for(j in 1:ncol(dist)){
      if(min[i]==dist[i,j]){
        ntst[i] = j
      }
    }
  }
  ntst
}
@
\subsection*{KPPV: fonctions d'apprentissage du nombre optimal de voisins K}
<<eval=FALSE>>=
kppv.app <- function(Xapp, zapp, Xval, zval, nppv)
{
  for(i in nppv){
    min <- i
    erreur_opt <- 1
    tmp <- kppv.val(Xapp, zapp, i, Xval)
    erreur <- sum((tmp == zval)==TRUE)/length(zval)
    if(erreur_opt > erreur){
      erreur_opt <- erreur
      min <- i
    }
  }
  min
}
@
\subsection*{KPPV: fonctions d'évaluation suivant un K donné}
<<eval=FALSE>>=
kppv.val <- function(Xapp, zapp, K, Xtst)
{
  dist <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(Xapp)){
      dist[j,i] <- sqrt(sum((Xtst[i,]-t(Xapp[j,]))^2))
    }
  }
  tmp <- apply(dist, 2, order)
  tmp2 <- tmp
  for(i in 1:ncol(tmp)){
    tmp2[,i] <- zapp[tmp[,i]]
  }
  tmp2 <- tmp2[1:K,]
  round(apply(tmp2, 2, mean))
}
@


\end{document}