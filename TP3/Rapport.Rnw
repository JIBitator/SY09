\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[cyr]{aeguill}
\usepackage[francais]{babel}
\usepackage{amsmath}

\begin{document}

\title{TP3: Discrimination, théorie bayésienne de la décision}
\author{Jean Baptiste Vivier - Marion Depuydt}
\maketitle
\section*{Classifieur euclidien, K plus proches voisins}



\subsection*{Programmation}

Voir code en annexe


\subsection*{Evaluation des performances}

\subsubsection*{Centre de gravité $ \mu_k $ et matrice de covariance $ \sum_k $ }
\section*{Règle de Bayes}

\subsection*{Distributions marginales}
X\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(\mu_1, \sum_1)$ et X\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(\mu_2, \sum_2)$
ce qui donne respectivement \newline
$f_1(x)= \frac{1}{2\pi}e^{-1/2*((x+2)^2+(y-1)^2)}$ et
$f_2(x)= \frac{1}{2\pi}e^{-1/2*((x-1)^2+(y-1)^2)}$ \newline
$\sum_1$ et $\sum_2$ sont diagonales ce qui implique que $X_1$, ..., $X_k$ sont indépendantes Nous pouvons par conséquent écrire que $f_1=f_{11}*f_{12}$ et $f_2=f_{21}*f_{22}$. \newline 
De plus, nous savons que les $f_{11}$ et $f_{12}$ sont des lois normales car ce sont des sous vecteurs d'un vecteur gaussien. \newline
Nous pouvons donc poser
$X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(a_1, b_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(a_2, b_2)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(c_1, d_1)$, $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(c_2, d_2)$. \newline
En résolvant le système nous obtenons: $X^1$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(-2, 1)$ , $X^1$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$ ainsi que $X^2$\raisebox{-0.7ex}{$\widetilde{w_1}$}$\mathcal{N}(1, 1)$ et $X^2$\raisebox{-0.7ex}{$\widetilde{w_2}$}$\mathcal{N}(1, 1)$

\subsection*{Courbes d'iso-densité}

L'équation de la courbe d'iso-densité de la classe 1 est $(x - \mu_1)^t\sum_1^{-1}(x-\mu_1)=c$ où c est une constante. Après avoir effectué les calculs nous obtenons, $(x-2)^2+(y-1)^2=c_1$ or l'équation d'un cercle de centre (a,b) et de rayon s'écrit $(x-a)^2+(y-b)^2=r^2$. 
Nous en déduisons donc que la courbe d'iso-densité de la classe 1 est un cercle de centre (-2,1) et de rayon $\sqrt{c_1}$.

Nous procédons de même pour déterminer que la courbe d'iso-densité de la classe 2 est un cercle de centre (1, 1) et de rayon $\sqrt{c_2}$.Nous remarquons que le centre des cercles correspond à $\mu$. 

Nous pouvons avoir une idée plus précise de la valeur $c_1$ en résolvant l'équation $f_1(x)=K_1$ où $K_1$ est une constante. Nous trouvons que $c_1=-2ln(2\pi K_1)$ et de la même manière $c_2=-2ln(2\pi K_2)$

\subsection*{Règle de Bayes}
La règle de Bayes s'écrit de la manière suivante:
$\delta^{*}(x) =
\begin{cases} 
  a_{1} & si \frac{f_{1}(x)}{f_{2}(x)} > \frac{\pi_{2}}{\pi{1}} \\
  a_{2} & sinon.
\end{cases}$\\ \\

Après simplification nous obtenons, $\delta^{*}(x) = \begin{cases} 
  a_{1} & si  \> x < \frac{ln(\pi_1)-ln(\pi_2)}{3} - \frac{1}{2} \\
  a_{2} & sinon.
\end{cases}$\\ \\

Or d'après l'énonce $\pi_1 = 0.5$ donc $\pi_2= 0.5$ aussi pour que la somme des proportions des classes soient égales à 1 ce qui simplifie l'équation:\\ 
$\delta^{*}(x) = \begin{cases} 
  a_{1} & si  \> x < - \frac{1}{2} \\
  a_{2} & sinon.
\end{cases}$\\ \\


\begin{verbatim}



\end{verbatim}
\section*{Annexe}

\section*{Règle de Bayes}

\subsection{quelles sont les distributions marginales des variables X1 et X2 dans chaque classe ?}

Les matrices de covariance conditionnelles $\Sigma_1$ et $\Sigma_2$ sont diagonales, donc les variables $X_1$ et $X_2$ sont indépendantes. Donc $f(x|w_k) = f(x_1|w_k)f(x_2|w_k), k \in {1,2}$

De plus, les individus des deux classes, $\omega_1$ et $\omega_2$ suivent une loi normale bivariée. Or tout sous-vecteur d'un vecteur aléatoire gaussien suit aussi une loi gaussienne. Les composantes de $X$, $X_1$ et $X_2$ suivent donc aussi une loi gaussienne.

En développant de part et d'autre l'égalité $f(x|w_k) = f(x_1|w_k)f(x_2|w_k), k \in {1,2}$, on obtient :

$X_1 \sim \mathcal{N}(\mu_{11}, 1)$ et
$X_2 \sim \mathcal{N}(\mu_{12}, 1)$

\subsection{Montrer que dans chaque classe, les courbes d’iso-densité sont des cercles dont on précisera lescentres et les rayons.}

$\Sigma_1$ et $\Sigma_2$ sont des matrice identité. Les courbes isodensité ont donc pour équation $(x-\mu_1)^t(x-\mu_1) = c_1$ et $(x-\mu_2)^t(x-\mu_2) = c_2$, où $c_1, c_2$ sont des constantes. 


Il s'agit d'équations de cercles dont les centres sont $\mu_1$ et $\mu_2$ respectivement, et de rayons $\sqrt{c_1}$ et $\sqrt{c_2}$ respectivement.



\section{Annexe}

\subsection*{Classifieur euclidien: fonction d'apprentissage}
<<eval=FALSE>>=
ceuc.app <- function(Xapp, zapp){
  apply(Xapp, 2, by, zapp, mean)
}
@
\subsection*{Classifieur euclidien: fonction d'évaluation}
<<eval=FALSE>>=
ceuc.val <- function(mu, Xtst){
  sol <- matrix(nrow = nrow(Xtst))
  dist <- matrix(ncol = nrow(mu), nrow = nrow(Xtst))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(mu)){
      dist[i,j] <- sqrt(sum((Xtst[i,]-t(mu[j,]))^2))
    }
  }
  # choisir la colonne de la classe
  min <- apply(dist, 1, min)
  ntst <- vector('numeric', nrow(dist))
  for(i in 1:nrow(dist)){
    for(j in 1:ncol(dist)){
      if(min[i]==dist[i,j]){
        ntst[i] = j
      }
    }
  }
  ntst
}
@
\subsection*{KPPV: fonctions d'apprentissage du nombre optimal de voisins K}
<<eval=FALSE>>=
kppv.app <- function(Xapp, zapp, Xval, zval, nppv)
{
  for(i in nppv){
    min <- i
    erreur_opt <- 1
    tmp <- kppv.val(Xapp, zapp, i, Xval)
    erreur <- sum((tmp == zval)==TRUE)/length(zval)
    if(erreur_opt > erreur){
      erreur_opt <- erreur
      min <- i
    }
  }
  min
}
@
\subsection*{KPPV: fonctions d'évaluation suivant un K donné}
<<eval=FALSE>>=
kppv.val <- function(Xapp, zapp, K, Xtst)
{
  dist <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
  for(i in 1:nrow(Xtst)){
    for(j in 1:nrow(Xapp)){
      dist[j,i] <- sqrt(sum((Xtst[i,]-t(Xapp[j,]))^2))
    }
  }
  tmp <- apply(dist, 2, order)
  tmp2 <- tmp
  for(i in 1:ncol(tmp)){
    tmp2[,i] <- zapp[tmp[,i]]
  }
  tmp2 <- tmp2[1:K,]
  round(apply(tmp2, 2, mean))
}
@


\end{document}